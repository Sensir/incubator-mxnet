# Adversarial examples

This demonstrates the concept of "adversarial examples" from [1] showing how to fool a well-trained CNN.
Adversarial examples are examples that humans can easily classify according to their true label but where the model is confused.
This specific adversarial example makes use of information of the gradient of the loss with respect to the input to craft adversarial examples.

[1] Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. "Explaining and harnessing adversarial examples." [arXiv preprint arXiv:1412.6572 (2014)](https://arxiv.org/abs/1412.6572)
